---
title: "Manipulación y agrupación de datos"
author: "Teresa Ortiz"
date: "Febrero 2015"
output:
  html_document:
    css: otros/cajas.css
    theme: spacelab
  pdf_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment=NA, fig.align="center")
options(digits = 4)
source("otros/tema_ggplot.R")
```

En estas notas continuamos con la introducción a R para análisis de datos, 
en particular mostraremos herramientas de manipulación de datos. Trataremos los
siguientes puntos:

* Reestructura de datos y el principio de los datos limpios.

* Estrategia divide-aplica-combina.

Para seguir estas notas debes descargar los datos de este  [link](https://www.dropbox.com/sh/c0mgho95gwjc1mv/AACVLPr33O6ENW68xmL7hyUna?dl=0)
y guardarlos en una carperta llamada *data*,  esta última debe estar en tu 
directorio de trabajo.

### Análisis de datos

En el análisis de datos nos interesan técnicas cuantitativas: cómo recolectar,
organizar, entender, interpretar y extraer información de colecciones de datos
predominantemente numéricos. Todas estas tareas son parte de un proceso que
podría resumirse con el siguiente diagrama (Hadley Wickham):

```{r, echo=FALSE, results="hide", message=FALSE}
library(plyr)
library(dplyr)
```

![](imagenes/analisis.png)

Para poder recorrer el diagrama suavemente es útil conocer las herramientas 
adecuadas de manipulación de datos, es sorprendente lo que unas cuantas 
funciones pueden ayudar. Comenzaremos con las herramientas de limpieza de 
datos pues este debe ser el primer paso de cualquier análisis.

### Datos limpios

Los **datos limpios** son datos que facilitan las tareas del análisis de datos: 

* **Manipulación**: Manipulación de variables como agregar, filtrar, reordenar,
transformar. 

*  **Visualización**: Resúmenes de datos usando gráficas, análisis exploratorio, 
o presentación de resultados. 

* **Modelación**: Ajustar modelos es sencillo si los datos están en la forma 
correcta.

Los principios de **datos limpios** ([Tidy Data de Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.pdf)) 
proveen una manera estándar de organizar la información:

1. Cada variable forma una columna.
2. Cada observación forma un renglón.
3. Cada tipo de unidad observacional forma una tabla.

Vale la pena notar que los principios de los datos limpios se pueden ver como 
teoría de algebra relacional para estadísticos.

Veamos un ejemplo:

La mayor parte de las bases de datos en estadística tienen forma rectangular, 
¿cuántas variables tiene la siguiente tabla?

||tratamientoA|tratamientoB
----|------------|---------
Juan Aguirre|- |2
Ana Bernal  |16|11
José López  |3 |1

La tabla anterior también se puede estructurar de la siguiente manera:

 ||Juan Aguirre| Ana Bernal|José López
--|------------|-----------|----------
tratamientoA|- |    16     |   3
tratamientoB|2 |    11     |   1


Si vemos los principios (cada variable forma una columna, cada observación forma 
un renglón, cada tipo de unidad observacional forma una tabla), 
¿las tablas anteriores cumplen los principios?

Para responder la pregunta veamos primero cuáles son las variables y cuáles
las observaciones de esta pequeña base. Las variables son: persona/nombre, tratamiento y resultado. Entonces, siguiendo los principios de _datos limpios_
obtenemos la siguiente estructura: 

nombre|tratamiento|resultado
------------|-----|---------
Juan Aguirre|a    |-
Ana Bernal  |a    |16
José López  |a    |3
Juan Aguirre|b    |2
Ana Bernal  |b    |11
José López  |b    |1

Una vez que identificamos los problemas de una base de datos podemos proceder a
la limpieza.

### Limpieza bases de datos
Algunos de los problemas más comunes en las bases de datos que no están 
_limpias_ son:

* Los encabezados de las columnas son valores y no nombres de variables. 
* Más de una variable por columna. 
* Las variables están organizadas tanto en filas como en columnas. 
* Más de un tipo de observación en una tabla.
* Una misma unidad observacional está almacenada en múltiples tablas. 

La mayor parte de estos problemas se pueden arreglar con pocas herramientas, 
a continuación veremos como _limpiar_ datos usando dos funciones del paquete
*tidyr* de Hadley Wickham:

* **gather**: recibe múltiples columnas y las junta en pares de nombres y 
valores, convierte los datos anchos en largos.  
* **spread**: recibe 2 columnas y las separa, haciendo los datos más anchos.

Repasaremos los problemas más comunes que se encuentran en conjuntos de datos
sucios y mostraremos como se puede manipular la tabla de datos (usando las 
funciones *gather* y *spread*) con el fin de estructurarla para que cumpla los
principios de datos limpios.

#### 1. Los encabezados de las columanas son valores
Usaremos ejemplos para entender los conceptos más facilmente.
La primer base de datos está basada en una encuesta de [Pew Research](http://www.pewforum.org/2009/01/30/income-distribution-within-us-religious-groups/) 
que investiga la relación entre ingreso y afiliación religiosa.

¿Cuáles son las variables en estos datos?

```{r}
# leemos la base
pew <- read.delim(file = "http://stat405.had.co.nz/data/pew.txt",
  header = TRUE, stringsAsFactors = FALSE, check.names = F)
pew
```

Esta base de datos tiene 3 variables: religión, ingreso y frecuencia. Para
_limpiarla_ es necesario apilar las columnas (alargar los datos). Notemos
que al alargar los datos desapareceran las columnas que se agrupan y dan lugar a
dos nuveas columnas: la correspondiente a clave y la correspondiente a valor.
Entonces, para alargar una base de datos usamos la función ´gather´ que recibe 
los argumentos:

* data: base de datos que vamos a reestructurar.  
* key: nombre de la nueva variable que contiene lo que fueron los nombres
de columnas que apilamos.  
* value: nombre de la variable que almacenará los valores que corresponden a 
cada *key*.  
* ...: lo último que especificamos son las columnas que vamos a apilar, veremos
que hay varias maneras de determinarlas.

```{r}
# cargamos el paquete
library(tidyr) 
library("dplyr")
pew_tidy <- gather(data = pew, income, frequency, -religion)
# vemos las primeras líneas de nuestros datos alargados 
head(pew_tidy) 
# y las últimas
tail(pew_tidy)
```

Observemos que en la tabla ancha teníamos bajo la columna *<$10k*, en el renglón
correspondiente a *Agnostic* un valor de 27, y podemos ver que este valor en 
la tabla larga se almacena bajo la columna frecuencia y corresponde a religión
*Agnostic*, income *<$10k*. También es importante ver que en este ejemplo 
especificamos las columnas a apilar identificando la que **no** vamos a alargar
con un signo negativo: es decir apila todas las columnas menos religión.

La nueva estructura de la base de datos nos permite, por ejemplo, hacer 
fácilmente una gráfica donde podemos comparar las diferencias en las 
frecuencias. 

Nota: En esta sección no explicaremos las funciones de graficación pues estas 
se cubren en las notas introductorias a R. En esta parte nos queremos concentrar
en como limpiar datos y ejemplificar lo sencillo que es trabajar con datos 
limpios, esto es, una vez que los datos fueron reestructurados es fácil 
construir gráficas y resúmenes.

```{r, fig.height = 5.8, fig.width = 6.8, warning = FALSE}
library(ggplot2)
ggplot(pew_tidy, aes(x = income, y = frequency, color = religion, 
  group = religion)) +
  geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Podemos hacer gráficas más interesantes si creamos nuevas variables:

```{r, fig.height = 4, fig.width = 7.7}
# explicaremos el operador %>% más adelante
by_religion <- group_by(pew_tidy, religion)
pew_tidy_2 <- pew_tidy %>%
  filter(income != "Don't know/refused") %>%
  group_by(religion) %>%
  mutate(percent = frequency / sum(frequency)) %>% 
  filter(sum(frequency) > 1000)

head(pew_tidy_2)

ggplot(pew_tidy_2, aes(x = income, y = percent, group = religion)) +
  facet_wrap(~ religion, nrow = 1) +
  geom_bar(stat = "identity", fill = "darkgray") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

En el código de arriba utilizamos las funciones `group_by`, `filter` y `mutate`
que estudiaremos más adelante. Por ahora concentremonos en `gather` y `spread`.

Otro ejemplo, veamos los datos de *Billboard*, aquí se registra la fecha en la 
que una canción entra por primera vez al top 100 de Billboard. 

```{r}
library(readr)
billboard <- read.csv("data/billboard.csv")
billboard
```

Notemos que el rank en cada semana (una vez que entró a la lista) está guardado
en 75 columnas `wk1` a `wk75`, este tipo de almacenamiento no es *limpio* pero 
puede ser útil al momento de ingresar la información.

Para tener datos *limpios* apilamos las semanas de manera que sea una sola 
columna (nuevamente alargamos los datos):

```{r}
billboard_long <- gather(billboard, week, rank, wk1:wk76, na.rm = TRUE)
billboard_long
```

Notemos que en esta ocasión especificamos las columnas que vamos a apilar
indicando el nombre de la primera de ellas seguido de `:` y por último el 
nombre de la última variable a apilar. Por otra parte, la instrucción 
`na.rm = TRUE` se utiliza para eliminar los renglones con valores faltantes en 
la columna de value (rank), esto es, eliminamos aquellas observaciones que 
tenían NA en la columnas wk*num* de la tabla ancha. Ahora realizamos una
limpieza adicional creando mejores variables de fecha.

```{r}
billboard_tidy <- billboard_long %>%
  mutate(
    week = extract_numeric(week),
    date = as.Date(date.entered) + 7 * (week - 1)) %>%
    select(-date.entered)
billboard_tidy
```

Nuevamente, es fácil hacer graficas.

```{r, fig.height = 3.8, fig.width = 7.7}
tracks <- filter(billboard_tidy, track %in% 
    c("Higher", "Amazed", "Kryptonite", "Breathe", "With Arms Wide Open"))

ggplot(tracks, aes(x = date, y = rank)) +
  geom_line() + 
  facet_wrap(~track, nrow = 1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### 2. Una columna asociada a más de una variable
La siguiente base de datos proviene de la Organización Mundial de la Salud y 
contiene el número de casos confirmados de tuberculosis por país y año, la
información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 
5-14, etc). 


```{r}
tb <- read_csv("data/tb.csv")
tb
```

![](imagenes/manicule2.jpg) De manera similar a los ejemplos anteriores, 
utiliza la función `gather` para apilar las columnas correspondientes a 
sexo-edad.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Piensa en 
como podemos separar la "variable" sexo-edad en dos columnas. 

```{r, echo=FALSE, results=FALSE}
tb_long <- gather(tb, demo, n, -iso2, -year, na.rm = TRUE)
tb_long
```

Ahora separaremos las variables sexo y edad de la columna demo, para ello 
debemos pasar a la función `separate`, esta recibe como parámetros:  

* el nombre de la base de datos,  

* el nombre de la variable que deseamos separar en más de una,  

* la posición de donde deseamos "cortar" (hay más opciones para especificar 
como separar, ver `?separate`)

```{r}
tb_tidy <- separate(tb_long, demo, c("sex", "age"), 8)
tb_tidy
table(tb_tidy$sex)

# creamos un mejor código de genero
tb_tidy <- mutate(tb_tidy, sex = substr(sex, 8, 8))
table(tb_tidy$sex)
```

#### 3. Variables almacenadas en filas y columnas
El problema más difícil es cuando las variables están tanto en filas como en 
columnas, veamos una base de datos de clima en Cuernavaca. ¿Cuáles son las 
variables en estos datos?

```{r}
clima <- tbl_df(read.delim("data/clima.txt", stringsAsFactors=FALSE))
clima
```

Estos datos tienen variables en columnas individuales (id, año, mes), en 
múltiples columnas (día, d1-d31) y en filas (tmin, tmax). Comencemos por apilar 
las columnas.

```{r}
clima_long <- gather(clima, day, value, d1:d31, na.rm = TRUE)
head(clima_long)
```

Podemos crear algunas variables adicionales.
```{r}
clima_vars <- clima_long %>% 
  mutate(day = extract_numeric(day), value = value / 10)  %>%
  select(id, year, month, day, element, value) %>%
  arrange(id, year, month, day)
head(clima_vars)
```

Finalmente, la columna *element* no es una variable, sino que almacena el nombre 
de dos variables, la operación que debemos aplicar (spread) es el inverso de 
apilar (gather):

```{r}
clima_tidy <- spread(clima_vars, element, value)
head(clima_tidy)
```

Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo.

```{r}
# ajustamos un modelo lineal donde la variable respuesta es temperatura 
# máxima, y la variable explicativa es el mes
clima_lm <- lm(TMAX ~ factor(month), data = clima_tidy)
summary(clima_lm)
```

#### 4. Mas de un tipo de observación en una misma tabla
En ocasiones las bases de datos involucran valores en diferentes niveles, en 
diferentes tipos de unidad observacional. En la limpieza de datos, cada unidad
observacional debe estar almacenada en su propia tabla (esto esta ligado a 
normalización de una base de datos), es importante para evitar inconsistencias 
en los datos.

¿Cuáles son las unidades observacionales de los datos de billboard?

```{r}
billboard_tidy
```

Separemos esta base de datos en dos: la tabla canción que almacena artista, 
nombre de la canción y duración; la tabla rank que almacena el ranking de la 
canción en cada semana.

```{r}
song <- billboard_tidy %>% 
  select(artist, track, year, time) %>%
  unique() %>%
  arrange(artist) %>%
  mutate(song_id = row_number(artist))
song

rank <- billboard_tidy %>%
  left_join(song, c("artist", "track", "year", "time")) %>%
  select(song_id, date, week, rank) %>%
  arrange(song_id, date) %>%
  tbl_df
rank
```

#### 5. Una misma unidad observacional está almacenada en múltiples tablas
También es común que los valores sobre una misma unidad observacional estén 
separados en muchas tablas o archivos, es común que estas tablas esten divididas 
de acuerdo a una variable, de tal manera que cada archivo representa a una 
persona, año o ubicación. Para juntar los archivos hacemos lo siguiente:

1. Leemos los archivos en una lista de tablas. 
2. Para cada tabla agregamos una columna que registra el nombre del archivo original. 
3. Combinamos las tablas en un solo data frame.  

Veamos un ejemplo, descarga la carpeta
[specdata](https://www.dropbox.com/sh/c0mgho95gwjc1mv/AACVLPr33O6ENW68xmL7hyUna?dl=0),
ésta contiene 332 archivos csv que almacenan información de monitoreo de 
contaminación en 332 ubicaciones de EUA. Cada archivo contiene información de 
una unidad de monitoreo y el número de identificación del monitor es el nombre 
del archivo.

Los pasos en R (usando el paquete `plyr`), primero creamos un vector con los
nombres de los archivos en un directorio, eligiendo aquellos que contengan las
letras ".csv".

```{r}
paths <- dir("data/specdata", pattern = "\\.csv$", full.names = TRUE)
```

Después le asignamos el nombre del csv al nombre de cada elemento del vector.
Este paso se realiza para preservar los nmobres de los archivos ya que estos
los asignaremos a una variable mas adelante.

```{r}
names(paths) <- basename(paths)
```

La función ldply del paquete plyr itera sobre cada dirección, lee el csv en 
dicha dirección y los combina en un data frame.

```{r}
library(plyr)

specdata_US <- tbl_df(ldply(paths, read.csv, stringsAsFactors = FALSE))
specdata_US

specdata <- specdata_US %>%
  mutate(
    monitor = extract_numeric(.id),
    date = as.Date(Date)) %>%
    select(id = ID, monitor, date, sulfate, nitrate)
```

#### 6. Otras consideraciones
En las buenas prácticas es importante tomar en cuenta los siguientes puntos:

* Incluir un encabezado con el nombre de las variables.
* Los nombres de las variables deben ser entendibles (e.g. AgeAtDiagnosis es mejor
que AgeDx).
* En general los datos se deben guardar en un archivo por tabla.
* Escribir un script con las modificaciones que se hicieron a los _datos crudos_ 
(reproducibilidad).
* Otros aspectos importantes en la _limpieza_ de datos son: selección del tipo de
variables (por ejemplo fechas), datos faltantes, _typos_ y detección de valores
atípicos.

***

### Divide-aplica-combina (_split-apply-combine_)
Muchos problemas de análisis de datos involucran la aplicación de la estrategia
divide-aplica-combina, ([Hadley Whickam, 2011](http://www.jstatsoft.org/v40/i01/paper)) 
esta consiste en romper un problema en pedazos (de 
acuerdo a una variable de interés), operar sobre cada subconjunto de manera
independiente (ej. calcular la media de cada grupo, ordenar observaciones por 
grupo, estandarizar por grupo) y después unir los pedazos nuevamente. El 
siguiente diagrama ejemplifiaca el paradigma de divide-aplica-combina:

* **Separa** la base de datos original.  
* **Aplica** funciones a cada subconjunto.  
* **Combina** los resultados en una nueva base de datos.

![](imagenes/split-apply-combine.png) 

En esta sección trabajaremos con las siguientes bases de datos para ejemplifcar
las funciones de divide-aplica-combina:

```{r}
flights <- tbl_df(read.csv("data/flights.csv", stringsAsFactors = FALSE))
flights
class(flights$date)
flights$date <- as.Date(flights$date)

weather <- tbl_df(read.csv("data/weather.csv", stringsAsFactors = FALSE))
weather$date <- as.Date(weather$date)
weather 

planes <- tbl_df(read_csv("data/planes.csv"))
planes

airports <- tbl_df(read.csv("data/airports.csv", stringsAsFactors = FALSE))
airports
```

Cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones, por ejemplo utilizar un ciclo _for_ para recorrer cada 
grupo de interés y aplicar las funciones, sin embargo la aplicación de ciclos 
_for_ desemboca en código difícil de entender. Adicionalmente, dplyr es mucho 
más veloz.

Estudiaremos las siguientes funciones:

* **filter**: obten un subconjunto de las filas de acuerdo a un criterio.
* **select**: selecciona columnas de acuerdo al nombre
* **arrange**: reordena las filas
* **mutate**: agrega nuevas variables
* **summarise**: reduce variables a valores (crear nuevas bases de datos)

Estas funciones trabajan de manera similar, el primer argumento que reciben 
es un _data frame_ (usualmente en formato *limpio*), los argumentos que siguen
indican que operación se va a efectuar y el resultado es un nuevo _data frame_.

Veamos con ejemplos.

#### Filtrar
Creamos una base de datos de juguete para mostrar el funcionamiento de cada
instrucción:

```{r}
df_ej <- data.frame(genero = c("mujer", "hombre", "mujer", "mujer", "hombre"), 
  estatura = c(1.65, 1.80, 1.70, 1.60, 1.67))
df_ej

filter(df_ej, genero == "mujer")
filter(df_ej, estatura > 1.65 & estatura < 1.75)
```

Algunos operadores importantes para filtrar son:  

```{r, eval = FALSE}
x > 1
x >= 1
x < 1
x <= 1
x != 1
x == 1
x %in% ("a", "b")

# Conjuntos
a | b
a & b
a & !b
xor(a, b)
```

![](imagenes/manicule2.jpg) Encuentra todos los vuelos hacia SFO ó OAK.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Los vuelos 
con un retraso mayor a una hora.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; En los que 
el retraso de llegada es más del doble que el retraso de salida.


#### Seleccionar
Elegir columnas de un conjunto de datos.
```{r}
library(dplyr)
df_ej
select(df_ej, genero)
select(df_ej, -genero)
```

```{r, eval = FALSE}
select(df_ej, starts_with("g"))
select(df_ej, contains("g"))
```

![](imagenes/manicule2.jpg) Ve la ayuda de select (`?select`) y escribe tres
maneras de seleccionar las variables de retraso (delay).

#### Arreglar
Arreglar u ordenar de acuerdo al valor de una o más variables:

```{r}
arrange(df_ej, genero)
arrange(df_ej, desc(estatura))
```

![](imagenes/manicule2.jpg) Ordena los vuelos por fecha de salida y hora.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ¿Cuáles
son los vuelos con mayor retraso?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ¿Qué vuelos 
_ganaron_ más tiempo en el aire?

#### Mutar
Mutar consiste en crear nuevas variables:
```{r}
mutate(df_ej, estatura_cm = estatura * 100) 
mutate(df_ej, estatura_cm = estatura * 100, estatura_in = estatura_cm * 0.3937) 
```
![](imagenes/manicule2.jpg) Calcula la velocidad en millas por hora a partir de
la variable tiempo y la distancia (en millas). ¿Quá vuelo fue el más rápido?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Crea una nueva
variable que muestre cuánto tiempo se ganó o perdió durante el vuelo.


#### Summarise y resúmenes por grupo
Summarise sirve para crear nuevas bases de datos con resúmenes o agregaciones de 
los datos originales.

```{r}
summarise(df_ej, promedio = mean(estatura))
```

Podemos hacer resúmenes por grupo, primero creamos una base de datos agrupada:

```{r}
by_genero <- group_by(df_ej, genero)
by_genero
```

y después operamos sobre cada grupo, creando un resumen a nivel grupo y uniendo
los subconjuntos en una base nueva:

```{r}
summarise(by_genero, promedio = mean(estatura))
```

![](imagenes/manicule2.jpg) Calcula el retraso promedio por fecha.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ¿Qué otros 
resúmenes puedes hacer para explorar el retraso por fecha?

* Algunas funciones útiles con _summarise_ son min(x), median(x), max(x), 
quantile(x, p), n(), sum(x), sum(x > 1), mean(x > 1), sd(x).

```{r}
by_date <- group_by(flights, date)
no_miss <- filter(by_date, !is.na(dep))
delays <- summarise(no_miss, 
  mean_delay = mean(dep_delay),
  n = n())
```


#### Operador pipeline
Cuando uno hace varias operaciones es difícil leer y entender el código: 

```{r}
hourly_delay <- filter(summarise(group_by(filter(flights, !is.na(dep_delay)), 
  date, hour), delay = mean(dep_delay), n = n()), n > 10)
```

La dificultad radica en que usualmente los parámetros se asignan después del 
nombre de la función usando (). El operador "Forward Pipe" (%>%) cambia este 
orden, manera que un parámetro que precede a la función es enviado ("piped") a 
la función.

Veamos como cambia el código anterior:

```{r}
hourly_delay <- flights %>%
  filter(!is.na(dep_delay)) %>%
  group_by(date, hour) %>%
  summarise(delay = mean(dep_delay), n = n()) %>%
  filter(n > 10)
```

podemos leer %>% como "_después_".

![](imagenes/manicule2.jpg) ¿Qué destinos tienen el promedio de retrasos más
alto?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ¿Qué vuelos 
(compañía + vuelo) ocurren diario?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; En promedio, 
¿Cómo varían a lo largo del día los retrasos de vuelos no cancelados? (pista: hour +
minute / 60)

#### Variables por grupo
En ocasiones es conveniente crear variables por grupo, por ejemplo estandarizar
dentro de cada grupo z = (x - mean(x)) / sd(x).

Veamos un ejemplo:
```{r}
planes <- flights %>%
  filter(!is.na(arr_delay)) %>%
  group_by(plane) %>%
  filter(n() > 30)

planes %>%
  mutate(z_delay =
    (arr_delay - mean(arr_delay)) / sd(arr_delay)) %>%
  filter(z_delay > 5)
```

#### Verbos de dos tablas
¿Cómo mostramos los retrasos de los vuelos en un mapa? 

Para responder esta pregunta necesitamos unir la base de datos de vuelos
con la de aeropuertos.

```{r}
location <- airports %>%
  select(dest = iata, name = airport, lat, long)

flights %>%
  group_by(dest) %>%
  filter(!is.na(arr_delay)) %>%
  summarise(
    arr_delay = mean(arr_delay),
    n = n() ) %>%
    arrange(desc(arr_delay)) %>%
    left_join(location) %>%
    tbl_df
```

Hay varias maneras de unir dos bases de datos y debemos pensar en el 
obejtivo:

```{r}
x <- data.frame(name = c("John", "Paul", "George", "Ringo", "Stuart", "Pete"),
  instrument = c("guitar", "bass", "guitar", "drums", "bass",
     "drums"))

y <- data.frame(name = c("John", "Paul", "George", "Ringo", "Brian"),
  band = c("TRUE", "TRUE", "TRUE",  "TRUE", "FALSE"))
x
y

inner_join(x, y)
left_join(x, y)
semi_join(x, y)
anti_join(x, y)
```

Resumamos lo que observamos arriba:

Tipo | Acción
-----|-------
inner|Incluye únicamente las filas que aparecen tanto en x como en y
left |Incluye todas las filas en x y las filas de y que coincidan
semi |Incluye las filas de x que coincidan con y
anti |Incluye las filas de x que no coinciden con y

Ahora combinamos datos a nivel hora con condiciones climáticas, ¿cuál es el tipo
de unión adecuado?

```{r}
hourly_delay <- flights %>%
  group_by(date, hour) %>%
  filter(!is.na(dep_delay)) %>%
  summarise(
    delay = mean(dep_delay),
    n = n() ) %>%
  filter(n > 10)

delay_weather <- hourly_delay %>% left_join(weather)

arrange(delay_weather, -delay)
```

![](imagenes/manicule2.jpg) ¿Qué condiciones climáticas están asociadad
con retrasos en las salidas de Houston?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Explora
si los aviones más viejos están asociados a mayores retrasos, responde
con una gráfica.

#### Métodos más generales (*Do*)

A diferencia de summarise, *do* es una función más lenta pero más general, 
resulta particularmente útil cuando se usa para ajustar un conjunto de modelos.

Veamos un ejemplo con la base de datos de bateo [Lahman](http://seanlahman.com/files/database/readme2012.txt). Agruparemos por 
año y ajustaremos un modelo para explorar la relación entre número de bateos y
carreras.

```{r}
batting <- read.csv("data/batting.csv")

batting_center <- batting %>% 
  group_by(yearID) %>%
  mutate(
    AB_center = AB - mean(AB, na.rm = TRUE)   # centramos dentro de cada grupo (año)
  ) 

models <- batting_center %>%
  group_by(yearID) %>%
  do(
    mod = lm(formula = R ~ AB_center, data = .)
  )
models

models[[1, 'mod']]
```

Podemos extraer componentes de los objetos lm guerdados en la variable mod.

```{r, fig.height = 4.5, fig.width = 4.8}
models_r <- models %>% 
  summarise(r.squared = summary(mod)$r.squared) 

models_2 <- models %>% 
  group_by(yearID) %>% 
  do(data.frame(t(coef(.[[1, 'mod']])))) %>%
  mutate(intercept = X.Intercept.) %>%
  select(-X.Intercept.)

models_3 <- cbind(models_2, models_r)


ggplot(models_3, aes(x = yearID, y = r.squared)) + geom_point() + geom_smooth()
```

Veamos las rectas de regresión.

```{r, fig.height = 4.5, fig.width = 4.8, warning = FALSE}
ggplot(batting, aes(x = AB, y = R, color = yearID, group = yearID)) +
  geom_smooth(method = "lm")
```

Podemos comparar los coeficientes

```{r, fig.width=4.8, fig.height=4.5}
models_3 <- gather(models_2, coef, value, -yearID)

ggplot(models_3, aes(x = yearID, y = value)) +
  geom_line() + 
  facet_wrap(~ coef, scales = "free_y", nrow = 2)
```

Podemos ver las diferencias a nivel año en los datos crudos.

```{r, fig.width=4, fig.height=4, warnings = FALSE}
ggplot(batting, aes(x = AB, y = R, color = yearID)) + 
  geom_point(alpha = 0.8, size = 1.6) 
```

Es conveniente usar el paquete biglm cuando se ajustan muchos modelos lineales, 
ya que guarda objetos de menor tamaño.

```{r}
library(biglm)
models_big <- batting %>%
  group_by(yearID) %>%
  do(
    mod = biglm(R ~ AB, data = .)
  )

print(object.size(models), unit = "MB")
print(object.size(models_big), unit = "MB")
```

En la función _do_ puede ser un poco complicado extraer las componentes de los 
objetos que ajustamos. Una alternativa es usar las funciones del paquete plyr 
(en particular ddply, ldply) o las funciones que vienen en R estándar (lapply, 
apply, tapply).

```{r}
models <- dlply(batting_center, "yearID", function(df){ 
  lm(R ~ AB_center, data = df)     # regresión lineal
})

models[[1]]

coef_list <- lapply(models, coef)
head(do.call(rbind, coef_list))
```


### Expresiones regulares

La expresiones regulares no son solo de R.

La idea detras de las expresiones regulares es poder parsear archivos de texto o texto en general. En nuestro contexto lo que buscamos es extraer datos de fuentes de texto diversas. Una expresion regular se puede pensar como una combinación de caracteres literales y meta caracteres. Los caracteres literales son de los que están formadas las palabras en el lenguaje utilizado.

* literales: "c",""o","n","a","b","i","o"
* metacaracteres: "*","?",".","|","^","$"

#### Operadores

* .: Cualquier simbolo
* [....]: Lista de caracteres, por ejemplo [Bb]iology10[1234] acepta cualquiera de las cadenas "Biology102", "biology101"
* \: Sirve para usar los metacaracteres como caracteres literales ($ * + . ? [ ] ^ { } | ( ) \), por ejemplo 10\^3 va aceptar la cadena "10^3"
* |: Operador "or" acepta un patron u otro, por ejemplo p(err|at)o va a aceptar tanto "perro" como "pato"
* (....): Grupos, sirven para recuperar partes del patrón encontrado para ser usadas después (se verá con ejemplo)

#### Cuantificadores

* *: Cero o más ocurrencias del caracter anterior, por ejemplo 10*, va a aceptar las cadenas "1", "10", "100", "1000", etc
* +: Una o más ocurrencias del caracter anterior, por ejemplo 10+, va a aceptar las cadenas 10", "100", "1000", etc, pero no la cadena "1"
* ?: Hasta una ocurrencia del caracter anterior, por ejemplo patos?, va aveptar las cadenas "pato" y "patos"
* {n}: Exactamente n veces el caracter anterior, por ejemplo 10{5}, únicamente va a aceptar la cadena "100000"
* {n,}: Mínimo n veces el caracter anterior, por ejemplo 10{5,}, aceptará las cadenas "100000", "1000000", "1000000", etc
* {n,m}: Entre n y m veces el caracter anterior, por ejemplo 10{2,5}, aceptará las cadenas "100", "10000"

### Expresiones regulares en R

Las expresiones regulares son independientes del lenguaje R, existe una implementación para prácticamente cualquier lenguaje de programación. En lo que nos compete en este curso, veremos los principales usos que se le pueden dar en la plataforma de interés. Para ejemplificar el uso de expresiones regulares en el lenguaje R se usarán datos procedentes de la siguiente página [homicides](http://data.baltimoresun.com/news/police/homicides)

```{r}
homicides <- readLines("data/homicides.txt")
head(homicides)
```

Podemos ver la naturaleza en la que vienen los datos.

```{r}
homicides[1]
homicides[1000]
```

#### grep, grepl


Jugando con la función grep podemos obtener el número de homicidios:

```{r}
length(grep("iconHomicideShooting", homicides))
length(grep("iconHomicideShooting|icon_homicide_shooting", homicides))
```

Usando otra sección de la información obtenemos:

```{r}
length(grep("Cause: shooting", homicides))
length(grep("Cause: [Ss]hooting", homicides))
```
```{r}
length(grep("[Ss]hooting", homicides))
i <- grep("[cC]ause: [sS]hooting", homicides)
j <- grep("[sS]hooting", homicides)
str(i)
str(j)
setdiff(i, j)
setdiff(j, i)
```

Analizando el resultado, vemos cual es la razón de dicha diferencia:

```{r}
homicides[859]
```

Así como podemos obtener los indices de las ocurrencias del patrón de interés, también podemos obtener los valores directamente, o un vector booleano con las incidencias (verdadero o falso) del patrón dentro de nuestro vector de cadenas.

```{r}
state.name
grep("^New", state.name)
grep("^New", state.name, value =TRUE)
grepl("^New", state.name)
```

#### regexpr, regmatches

```{r}
homicides[1]
regexpr("<dd>[F|f]ound.*</dd>", homicides[1:5])
substr(homicides[1], 177, 177 + 93 -1)
regexpr("<dd>[F|f]ound.*?</dd>", homicides[1:5])
substr(homicides[1], 177, 177 + 33 -1)
r <- regexpr("<dd>[F|f]ound.*?</dd>", homicides[1:5])
regmatches(homicides[1:5], r)
x <- substr(homicides[1], 177, 177 + 33 -1)
x
```

#### sub, gsub

```{r}
sub("<dd>[F|f]ound on |</dd>", "", x)
gsub("<dd>[F|f]ound on |</dd>", "", x)
r <- regexpr("<dd>[F|f]ound.*?</dd>", homicides[1:5])
regmatches(homicides[1:5], r)
m <-regmatches(homicides[1:5], r)
gsub("<dd>[F|f]ound on |</dd>", "", m)
d <- gsub("<dd>[F|f]ound on |</dd>", "", m)
as.Date(d, "%B %d, %Y")
```

#### regexec

```{r}
regexec("<dd>[F|f]ound on (.*?)</dd>", homicides[1:5])
regexec("<dd>[F|f]ound on .*?</dd>", homicides[1:5])
substr(homicides[1:5], 177, 177 + 93 -1)
substr(homicides[1:5], 177, 177 + 33 -1)
substr(homicides[1:5], 190, 190 + 15 -1)
r <- regexec("<dd>[F|f]ound on (.*?)</dd>", homicides[1:5])
regmatches(homicides[1:5], r)
```

#### Todo junto

```{r}
r <- regexec("<dd>[F|f]ound on (.*?)</dd>", homicides)
m <- regmatches(homicides, r)
dates <- sapply(m, function(x) x[2])
dates <- as.Date(dates, "%B %d, %Y")
hist(dates, "month", freq = TRUE)
```



### Recursos adicionales

* [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf), Hadley Wickham.

* [The Slit-Apply-Combine Strategy for Data Analysis](http://www.jstatsoft.org/v40/i01/paper), 
Hadley Wickham 2011.

* [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), 
RStudio.


